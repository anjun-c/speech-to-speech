{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c8183e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pypdf\n",
    "import docx2txt\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from config import settings\n",
    "\n",
    "class FileProcessor:\n",
    "    \"\"\"Utility class for processing different file types and creating vector stores.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the file processor with default text splitter and embeddings.\"\"\"\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "        )\n",
    "        # Using a lightweight embedding model that doesn't require GPU\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"Qwen3-Embedding-8B\",\n",
    "            model_kwargs={'device': 'gpu' if settings.use_gpu else 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        \n",
    "    def is_valid_extension(self, filename: str) -> bool:\n",
    "        \"\"\"Check if the file has an allowed extension.\"\"\"\n",
    "        return filename.split('.')[-1].lower() in settings.allowed_extensions\n",
    "    \n",
    "    def extract_text_from_file(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from different file types.\"\"\"\n",
    "        file_extension = file_path.split('.')[-1].lower()\n",
    "        \n",
    "        if file_extension == 'pdf':\n",
    "            return self._extract_text_from_pdf(file_path)\n",
    "        elif file_extension == 'docx':\n",
    "            return self._extract_text_from_docx(file_path)\n",
    "        elif file_extension == 'txt':\n",
    "            return self._extract_text_from_txt(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "    \n",
    "    def _extract_text_from_pdf(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF files.\"\"\"\n",
    "        text = \"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = pypdf.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    \n",
    "    def _extract_text_from_docx(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from DOCX files.\"\"\"\n",
    "        return docx2txt.process(file_path)\n",
    "    \n",
    "    def _extract_text_from_txt(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from TXT files.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    \n",
    "    def process_file(self, file_path: str, file_id: Optional[str] = None) -> str:\n",
    "        \"\"\"Process a file and add it to the vector store.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        if not self.is_valid_extension(file_path):\n",
    "            raise ValueError(f\"Invalid file extension for file: {file_path}\")\n",
    "        \n",
    "        # Extract text from the file\n",
    "        text = self.extract_text_from_file(file_path)\n",
    "        \n",
    "        # Split text into chunks\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        \n",
    "        # Generate a unique ID for this document if not provided\n",
    "        if file_id is None:\n",
    "            file_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Create metadata for each chunk\n",
    "        metadatas = [{\"source\": file_path, \"file_id\": file_id} for _ in chunks]\n",
    "        \n",
    "        # Create or update the vector store\n",
    "        faiss_index_path = os.path.join(settings.vector_store_dir, \"faiss_index\")\n",
    "        \n",
    "        # Check if existing FAISS index exists\n",
    "        if os.path.exists(faiss_index_path):\n",
    "            # Load existing vector store and add new documents\n",
    "            vector_store = FAISS.load_local(\n",
    "                faiss_index_path,\n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            vector_store.add_texts(texts=chunks, metadatas=metadatas)\n",
    "        else:\n",
    "            # Create new vector store\n",
    "            vector_store = FAISS.from_texts(\n",
    "                texts=chunks,\n",
    "                embedding=self.embeddings,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "        \n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(settings.vector_store_dir, exist_ok=True)\n",
    "        \n",
    "        # Save the vector store\n",
    "        vector_store.save_local(faiss_index_path)\n",
    "        \n",
    "        return file_id\n",
    "    \n",
    "    def save_uploaded_file(self, file_content: bytes, filename: str) -> str:\n",
    "        \"\"\"Save an uploaded file to the data directory and return the file path.\"\"\"\n",
    "        # Generate a unique filename to avoid collisions\n",
    "        unique_filename = f\"{uuid.uuid4()}_{filename}\"\n",
    "        file_path = os.path.join(settings.data_dir, unique_filename)\n",
    "        \n",
    "        # Save the file\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(file_content)\n",
    "        \n",
    "        return file_path\n",
    "    \n",
    "    def get_vector_store(self):\n",
    "        \"\"\"Get the vector store for querying.\"\"\"\n",
    "        faiss_index_path = os.path.join(settings.vector_store_dir, \"faiss_index\")\n",
    "        \n",
    "        # Check if FAISS index exists\n",
    "        if os.path.exists(faiss_index_path):\n",
    "            return FAISS.load_local(\n",
    "                faiss_index_path,\n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "        else:\n",
    "            # Return empty FAISS vector store if no index exists\n",
    "            return FAISS.from_texts(\n",
    "                texts=[\"\"],\n",
    "                embedding=self.embeddings,\n",
    "                metadatas=[{\"source\": \"empty\", \"file_id\": \"empty\"}]\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
